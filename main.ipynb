{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://data.deepai.org/mnist.zip -o mnist.zip\n",
    "!unzip mnist.zip -d mnist/\n",
    "!rm mnist.zip\n",
    "!gunzip mnist -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "device = 'cpu'\n",
    "\n",
    "def load_mnist_data(test=False):\n",
    "    if(test):\n",
    "        f_images = open('mnist/t10k-images-idx3-ubyte','rb')\n",
    "        f_labels = open('mnist/t10k-labels-idx1-ubyte','rb')\n",
    "    else:\n",
    "        f_images = open('mnist/train-images-idx3-ubyte','rb')\n",
    "        f_labels = open('mnist/train-labels-idx1-ubyte','rb')\n",
    "        \n",
    "    # skip bullshit start\n",
    "    f_images.seek(16)\n",
    "    f_labels.seek(8)\n",
    "    \n",
    "    # read whole file\n",
    "    buf_images = f_images.read()\n",
    "    buf_labels = f_labels.read()\n",
    "    \n",
    "    images = np.frombuffer(buf_images, dtype=np.uint8).astype(np.float32)\n",
    "    images = images.reshape(-1, 1, 28, 28) / 256\n",
    "    \n",
    "    labels = np.frombuffer(buf_labels, dtype=np.uint8)\n",
    "    labels_one_hot = np.zeros((labels.shape[0], 10))\n",
    "    labels_one_hot[np.arange(labels.size), labels] = 1\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "def sample_batch(X, Y, batch_size=32):\n",
    "    length = len(Y)\n",
    "    idx = np.random.choice(np.arange(0, length), size=(batch_size), replace=False)\n",
    "    \n",
    "    return X[idx], Y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checker_board(d_model):\n",
    "    half = (d_model) // 2\n",
    "    texture = torch.cat([\n",
    "        torch.ones((half, 1)),\n",
    "        torch.zeros((half, 1))\n",
    "    ], dim=1).view((-1,))\n",
    "    \n",
    "    return texture\n",
    "\n",
    "def pos_embedding(x):\n",
    "    device = x.device\n",
    "    # x: (pos, n, i)\n",
    "\n",
    "    length = x.shape[0]\n",
    "    batch_size = x.shape[1]\n",
    "    d_model = x.shape[2]\n",
    "\n",
    "    i = torch.arange(0, d_model).view((1, 1, -1)).expand(length, -1, d_model).to(device).float()\n",
    "    pos = torch.arange(0, length).view((-1, 1, 1)).expand(length, -1, d_model).to(device).float()\n",
    "\n",
    "    z = pos / 10000 ** (i / d_model)\n",
    "\n",
    "    sin = torch.sin(z)\n",
    "    cos = torch.cos(z)\n",
    "\n",
    "    sin_mask = checker_board(d_model).to(device)\n",
    "    cos_mask = -sin_mask + 1\n",
    "\n",
    "    pe = (sin_mask * sin) + (cos_mask * cos)\n",
    "    pe = pe.expand(length, batch_size, d_model)\n",
    "\n",
    "    return x + pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        \n",
    "        self.layer_norm_x = nn.LayerNorm([d_model])\n",
    "        self.layer_norm_1 = nn.LayerNorm([d_model])\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            d_model,\n",
    "            heads,\n",
    "            dropout=0.0,\n",
    "            bias=True,\n",
    "            add_bias_kv=True,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, d_model)\n",
    "        self.layer_norm_2 = nn.LayerNorm([d_model])\n",
    "        self.linear2 = nn.Linear(d_model, d_model)\n",
    "        self.linear3 = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x, z_input):\n",
    "        x = self.layer_norm_x(x)\n",
    "        z = self.layer_norm_1(z_input)\n",
    "        z, _ = self.attention(z, x, x)\n",
    "        \n",
    "        z = self.dropout(z)\n",
    "        z = self.linear1(z)\n",
    "        \n",
    "        z = self.layer_norm_2(z)\n",
    "        z = self.linear2(z)\n",
    "        z = F.gelu(z)\n",
    "        z = self.dropout(z)\n",
    "        z = self.linear3(z)\n",
    "        \n",
    "        return z + z_input\n",
    "\n",
    "class PerceiverBlock(nn.Module):\n",
    "    def __init__(self, d_model, latent_blocks, dropout, heads):\n",
    "        super(PerceiverBlock, self).__init__()\n",
    "        \n",
    "        self.cross_attention = AttentionBlock(d_model, heads=1, dropout=dropout)\n",
    "        self.latent_attentions = nn.ModuleList([\n",
    "            AttentionBlock(d_model, heads=heads, dropout=dropout) for _ in range(latent_blocks)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x, z):\n",
    "        z = self.cross_attention(x, z)\n",
    "        for latent_attention in self.latent_attentions:\n",
    "            z = latent_attention(z, z)\n",
    "        return z\n",
    "\n",
    "class Repeater(nn.Module):\n",
    "    def __init__(self, module, repeats=1):\n",
    "        super(Repeater, self).__init__()\n",
    "        \n",
    "        self.repeats = repeats\n",
    "        self.module = module\n",
    "    \n",
    "    def forward(self, x, z):\n",
    "        for _ in range(self.repeats):\n",
    "            z = self.module(x, z)\n",
    "        return z\n",
    "    \n",
    "class Perceiver(nn.Module):\n",
    "    def __init__(self, input_channels, output_features, latents=64, d_model=32, heads=8, dropout=0.1, layers=8):\n",
    "        super(Perceiver, self).__init__()\n",
    "\n",
    "        self.init_latent = nn.Parameter(torch.rand((latents, d_model)))\n",
    "        self.embedding = nn.Conv1d(input_channels, d_model, 1)\n",
    "        \n",
    "        self.block1 = Repeater(PerceiverBlock(d_model, latent_blocks=6, heads=heads, dropout=dropout), repeats=1) # 1\n",
    "        self.block2 = Repeater(PerceiverBlock(d_model, latent_blocks=6, heads=heads, dropout=dropout), repeats=max(layers - 1, 0)) # 2-8\n",
    "        \n",
    "        self.linear1 = nn.Linear(d_model, d_model)\n",
    "        self.linear2 = nn.Linear(d_model, output_features)   \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Transform our X (input)     \n",
    "        # x.shape = (batch_size, channels, width, height)\n",
    "        x = x.view((x.shape[0], x.shape[1], -1))\n",
    "        # x.shape = (batch_size, channels, pixels)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        # x.shape = (batch_size, d_model, pixels)\n",
    "        x = x.permute(2, 0, 1)\n",
    "        # x.shape (pixels, batch_size, d_model)\n",
    "        \n",
    "        x = pos_embedding(x)\n",
    "               \n",
    "        # Transform our Z (latent)\n",
    "        # z.shape = (latents, d_model)\n",
    "        z = self.init_latent.unsqueeze(1)\n",
    "        # z.shape = (latents, 1, d_model)\n",
    "        z = z.expand(-1, x.shape[1], -1)\n",
    "        # z.shape = (latents, batch_size, d_model)\n",
    "        \n",
    "        z = self.block1(x, z)\n",
    "        z = self.block2(x, z)\n",
    "        \n",
    "        z = self.linear1(z)\n",
    "        z = z.mean(dim=0)\n",
    "        z = self.linear2(z)\n",
    "        \n",
    "        return z\n",
    "    \n",
    "model = Perceiver(input_channels=1, output_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_mnist_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6c4eefe740dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-6c4eefe740dd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, SKIP_EPOCHS, EPOCHS, BATCH_SIZE, DEVICE)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_mnist_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mX_LENGTH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_mnist_data' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "def test(model, DEVICE='cpu'):\n",
    "    model.eval()\n",
    "    model = model.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        X_test, Y_test = load_mnist_data(test=True)\n",
    "        X_LENGTH = len(X_test) // 10\n",
    "        BATCH_SIZE = 500\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        t = range(X_LENGTH // BATCH_SIZE)\n",
    "        for i in t:\n",
    "            x = torch.from_numpy(X_test[i * BATCH_SIZE:(i+1) * BATCH_SIZE]).float().to(DEVICE)\n",
    "            y = torch.from_numpy(Y_test[i * BATCH_SIZE:(i+1) * BATCH_SIZE]).long().to(DEVICE)\n",
    "\n",
    "            y_ = model(x).argmax(dim=-1)\n",
    "\n",
    "            total += len(y_)\n",
    "            correct += (y_ == y).sum().item()\n",
    "\n",
    "        return correct / total\n",
    "    \n",
    "def train(model, SKIP_EPOCHS=-1, EPOCHS=10, BATCH_SIZE=64, DEVICE='cpu'):\n",
    "    model.train()\n",
    "    model = model.to(DEVICE)\n",
    "    gamma = 0.1 ** 0.5 # 0.3ish\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.004)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=gamma, last_epoch=-1, verbose=False)\n",
    "\n",
    "    X_train, Y_train = load_mnist_data(test=False)\n",
    "    X_LENGTH = len(X_train)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print('EPOCH', epoch, '[LEARNING RATE: ' + str(optimizer.param_groups[0]['lr']) + '; ACCURACY: ' + str(test(model, DEVICE=DEVICE)) + ']')\n",
    "        if(epoch <= SKIP_EPOCHS):\n",
    "            scheduler.step()\n",
    "            continue\n",
    "\n",
    "        t = trange(X_LENGTH // BATCH_SIZE)\n",
    "        for _ in t:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x, y = sample_batch(X_train, Y_train, BATCH_SIZE)\n",
    "            x = torch.from_numpy(x).float().to(DEVICE)\n",
    "            y = torch.from_numpy(y).long().to(DEVICE)\n",
    "\n",
    "            y_ = model(x)\n",
    "            loss = nn.CrossEntropyLoss()(y_, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            t.set_description(str(loss.item())[0:5])\n",
    "        scheduler.step()\n",
    "        \n",
    "train(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "imshow(data[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
