{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://data.deepai.org/mnist.zip -o mnist.zip\n",
    "!unzip mnist.zip -d mnist/\n",
    "!rm mnist.zip\n",
    "!gunzip mnist -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t10k-images-idx3-ubyte  train-images-idx3-ubyte\r\n",
      "t10k-labels-idx1-ubyte  train-labels-idx1-ubyte\r\n"
     ]
    }
   ],
   "source": [
    "!ls mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_mnist_data(test=False):\n",
    "    if(test):\n",
    "        f_images = open('mnist/t10k-images-idx3-ubyte','rb')\n",
    "        f_labels = open('mnist/t10k-labels-idx1-ubyte','rb')\n",
    "    else:\n",
    "        f_images = open('mnist/train-images-idx3-ubyte','rb')\n",
    "        f_labels = open('mnist/train-labels-idx1-ubyte','rb')\n",
    "        \n",
    "    # skip bullshit start\n",
    "    f_images.seek(16)\n",
    "    f_labels.seek(8)\n",
    "    \n",
    "    # read whole file\n",
    "    buf_images = f_images.read()\n",
    "    buf_labels = f_labels.read()\n",
    "    \n",
    "    images = np.frombuffer(buf_images, dtype=np.uint8).astype(np.float32)\n",
    "    images = images.reshape(-1, 1, 28, 28) / 256\n",
    "    \n",
    "    labels = np.frombuffer(buf_labels, dtype=np.uint8)\n",
    "    labels_one_hot = np.zeros((labels.shape[0], 10))\n",
    "    labels_one_hot[np.arange(labels.size), labels] = 1\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "def sample_batch(X, Y, batch_size=32):\n",
    "    length = len(Y)\n",
    "    idx = np.random.choice(np.arange(0, length), size=(batch_size), replace=False)\n",
    "    \n",
    "    return X[idx], Y[idx]\n",
    "\n",
    "## USAGE EXAMPLE:\n",
    "#  X, Y = load_mnist_data(test=False)\n",
    "#  x, y = sample_batch(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage of BLOCKS:\n",
    "# - BLOCK A * 1\n",
    "# - BLOCK B * 7\n",
    "\n",
    "# BLOCK:\n",
    "# - Cross Attention with only 1 head (Q from latent, KV from input)\n",
    "# - Latent Transformer with X blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checker_board(d_model):\n",
    "    half = (d_model) // 2\n",
    "    texture = torch.cat([\n",
    "        torch.ones((half, 1)),\n",
    "        torch.zeros((half, 1))\n",
    "    ], dim=1).view((-1,))\n",
    "    \n",
    "    return texture\n",
    "\n",
    "def pos_embedding(x):\n",
    "        # x: (pos, n, i)\n",
    "        \n",
    "        length = x.shape[0]\n",
    "        batch_size = x.shape[1]\n",
    "        d_model = x.shape[2]\n",
    "\n",
    "        i = torch.arange(0, d_model).view((1, 1, -1)).expand(length, -1, d_model).to(device).float()\n",
    "        pos = torch.arange(0, length).view((-1, 1, 1)).expand(length, -1, d_model).to(device).float()\n",
    "        \n",
    "        z = pos / 10000 ** (i / d_model)\n",
    "        \n",
    "        sin = torch.sin(z)\n",
    "        cos = torch.cos(z)\n",
    "        \n",
    "        sin_mask = checker_board(d_model).to(device)\n",
    "        cos_mask = -sin_mask + 1\n",
    "                \n",
    "        pe = (sin_mask * sin) + (cos_mask * cos)\n",
    "        pe = pe.expand(length, batch_size, d_model)\n",
    "        \n",
    "        return x + pe\n",
    "    \n",
    "#pos_embedding(torch.ones((16, 1, 8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.25):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        \n",
    "        self.layer_norm1 = nn.LayerNorm([d_model])\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            d_model,\n",
    "            heads,\n",
    "            dropout=0.0,\n",
    "            bias=True,\n",
    "            add_bias_kv=True,\n",
    "        )\n",
    "        #self.dropout = nn.Dropout(p=dropout)\n",
    "        #self.linear1 = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        #self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.linear2 = nn.Linear(d_model, d_model)\n",
    "        #self.linear3 = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x, z_input):\n",
    "        x = self.layer_norm1(x)\n",
    "        z = self.layer_norm1(z_input)\n",
    "        z, _ = self.attention(z, x, x)\n",
    "        \n",
    "        #z = self.dropout(z)\n",
    "        #z = self.linear1(z)\n",
    "        \n",
    "        #z = self.layer_norm2(z)\n",
    "        z = self.linear2(z)\n",
    "        z = F.gelu(z)\n",
    "        #z = self.dropout(z)\n",
    "        #z = self.linear3(z)\n",
    "        \n",
    "        return z + z_input\n",
    "\n",
    "class PerceiverBlock(nn.Module):\n",
    "    def __init__(self, d_model, latent_blocks):\n",
    "        super(PerceiverBlock, self).__init__()\n",
    "        \n",
    "        self.cross_attention = AttentionBlock(d_model, heads=1)\n",
    "        self.latent_attentions = nn.ModuleList([\n",
    "            AttentionBlock(d_model, heads=4) for _ in range(latent_blocks)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x, z):\n",
    "        z = self.cross_attention(x, z)\n",
    "        #for latent_attention in self.latent_attentions:\n",
    "        #    z = latent_attention(z, z)\n",
    "        return z\n",
    "\n",
    "class Repeater(nn.Module):\n",
    "    def __init__(self, module, repeats=1):\n",
    "        super(Repeater, self).__init__()\n",
    "        \n",
    "        self.repeats = repeats\n",
    "        self.module = module\n",
    "    \n",
    "    def forward(self, x, z):\n",
    "        for _ in range(self.repeats):\n",
    "            z = self.module(x, z)\n",
    "        return z\n",
    "    \n",
    "class Perceiver(nn.Module):\n",
    "    def __init__(self, output_size, latents=16, d_model=8, input_channels=1):\n",
    "        super(Perceiver, self).__init__()\n",
    "\n",
    "        self.init_latent = nn.Parameter(torch.rand((latents, d_model)))\n",
    "        self.embedding = nn.Conv1d(input_channels, d_model, 1)\n",
    "        \n",
    "        self.block1 = Repeater(PerceiverBlock(d_model, latent_blocks=2), repeats=1)\n",
    "        self.block2 = Repeater(PerceiverBlock(d_model, latent_blocks=2), repeats=2)\n",
    "        self.block3 = Repeater(PerceiverBlock(d_model, latent_blocks=2), repeats=2)\n",
    "        \n",
    "        self.linear1 = nn.Linear(d_model, d_model)\n",
    "        self.linear2 = nn.Linear(d_model, output_size)   \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Transform our X (input)     \n",
    "        # x.shape = (batch_size, channels, width, height)\n",
    "        x = x.view((x.shape[0], x.shape[1], -1))\n",
    "        # x.shape = (batch_size, channels, pixels)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        # x.shape = (batch_size, d_model, pixels)\n",
    "        x = x.permute(2, 0, 1)\n",
    "        # x.shape (pixels, batch_size, d_model)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = pos_embedding(x)\n",
    "        \n",
    "        #imshow(x.detach().numpy() * 255)\n",
    "        \n",
    "               \n",
    "        #raise False\n",
    "               \n",
    "        # Transform our Z (latent)\n",
    "        # z.shape = (latents, d_model)\n",
    "        z = self.init_latent.unsqueeze(1)\n",
    "        # z.shape = (latents, 1, d_model)\n",
    "        z = z.expand(-1, x.shape[1], -1)\n",
    "        # z.shape = (latents, batch_size, d_model)\n",
    "        \n",
    "        #print('0', z)\n",
    "        z = self.block1(x, z)\n",
    "        #print('1', z)\n",
    "        z = self.block2(x, z)\n",
    "        z = self.block3(x, z)\n",
    "        #print('2', z)\n",
    "        \n",
    "        z = self.linear1(z)\n",
    "        z = z.mean(dim=0)\n",
    "        z = self.linear2(z)\n",
    "        \n",
    "        return z\n",
    "    \n",
    "model = Perceiver(output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.101"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test(model):\n",
    "    with torch.no_grad():\n",
    "        X_test, Y_test = load_mnist_data(test=True)\n",
    "        X_LENGTH = len(X_test)\n",
    "        BATCH_SIZE = 500\n",
    "        DEVICE = 'cpu'\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        t = range(X_LENGTH // BATCH_SIZE)\n",
    "        for i in t:\n",
    "            x = torch.from_numpy(X_test[i * BATCH_SIZE:(i+1) * BATCH_SIZE]).float().to(DEVICE)\n",
    "            y = torch.from_numpy(Y_test[i * BATCH_SIZE:(i+1) * BATCH_SIZE]).long().to(DEVICE)\n",
    "\n",
    "            y_ = model(x).argmax(dim=-1)\n",
    "\n",
    "            total += len(y_)\n",
    "            correct += (y_ == y).sum().item()\n",
    "\n",
    "        return correct / total\n",
    "    \n",
    "test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1, last_epoch=-1, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2.295:   0%|          | 2/1875 [00:00<02:11, 14.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 [LEARNING RATE: 0.01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.308: 100%|██████████| 1875/1875 [01:54<00:00, 16.33it/s]\n",
      "1.184:   0%|          | 2/1875 [00:00<02:14, 13.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ACCURACY: 0.6203]\n",
      "EPOCH 1 [LEARNING RATE: 0.01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.535: 100%|██████████| 1875/1875 [01:59<00:00, 15.75it/s]\n",
      "1.314:   0%|          | 2/1875 [00:00<02:25, 12.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ACCURACY: 0.7496]\n",
      "EPOCH 2 [LEARNING RATE: 0.01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.406:  57%|█████▋    | 1062/1875 [01:10<00:52, 15.51it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "X, Y = load_mnist_data(test=False)\n",
    "X_LENGTH = len(X)\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH', epoch, '[LEARNING RATE: ' + str(optimizer.param_groups[0]['lr']) + ']')\n",
    "    if(epoch <= -1):\n",
    "        scheduler.step()\n",
    "        continue\n",
    "        \n",
    "    t = trange(X_LENGTH // BATCH_SIZE)\n",
    "    for _ in t:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x, y = sample_batch(X, Y, BATCH_SIZE)\n",
    "        x = torch.from_numpy(x).float().to(DEVICE)\n",
    "        y = torch.from_numpy(y).long().to(DEVICE)\n",
    "        \n",
    "        y_ = model(x)\n",
    "        loss = nn.CrossEntropyLoss()(y_, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        t.set_description(str(loss.item())[0:5])\n",
    "    print('[ACCURACY: ' + str(test(model)) + ']')\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0139, -0.1574,  0.5565,  0.2240, -0.1797, -0.2278,  0.2837,  0.0194,\n",
       "         -0.0857, -0.2254]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([0.0,1.0]).view((1,1,1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "X, Y = load_mnist_data(test=False)\n",
    "X_LENGTH = len(X)\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "for epoch in range(1):\n",
    "    for _ in range(10000):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x = torch.round(torch.rand((BATCH_SIZE, 1, 1, 2)))\n",
    "        #print(x)\n",
    "        y = torch.prod(x, dim=3)\n",
    "        \n",
    "        y_ = model(x)\n",
    "        loss = nn.MSELoss()(y_, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if(_ % 2 == 0):\n",
    "            print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.4986,  0.8630,  0.2013,  0.3818,  0.7835,  0.8413,  0.1154,  0.8566,\n",
       "          0.5011,  0.8450,  0.5101,  0.7268,  0.8154,  0.8392,  0.7373,  0.1881],\n",
       "        [ 0.1795,  0.6890,  0.4758,  0.4538,  0.9585,  0.7544,  0.7605,  0.1170,\n",
       "          1.0515,  0.6272,  0.6923,  0.1439,  0.3151,  1.0141,  0.6098,  0.4420],\n",
       "        [ 0.3529,  0.4465,  0.5990,  0.9882,  0.6056,  0.4392,  0.9566,  0.6223,\n",
       "          0.5232,  0.4905,  0.3253, -0.0067,  0.7748,  0.0487,  0.4261,  0.0225],\n",
       "        [ 0.0740,  0.6972, -0.0544,  0.7206,  0.3876,  0.2320,  0.5455,  0.1779,\n",
       "          0.0595,  0.5255,  0.9128,  0.9452,  0.0945,  0.7067,  0.3905,  0.5256]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.init_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.5033,  0.8624,  0.2247,  0.3716,  0.7758,  0.8854,  0.1261,  0.8666,\n",
       "          0.5142,  0.8339,  0.4869,  0.7186,  0.8476,  0.8083,  0.7432,  0.1994],\n",
       "        [ 0.1934,  0.6917,  0.4956,  0.4365,  0.9429,  0.8464,  0.7469,  0.1681,\n",
       "          1.0636,  0.6288,  0.6425,  0.1440,  0.3613,  0.9735,  0.6283,  0.4511],\n",
       "        [ 0.3527,  0.4468,  0.6180,  0.9728,  0.5977,  0.4942,  0.9471,  0.6464,\n",
       "          0.5252,  0.5152,  0.2958, -0.0106,  0.8017,  0.0276,  0.4370,  0.0278],\n",
       "        [ 0.0616,  0.7055, -0.0422,  0.7026,  0.3713,  0.2474,  0.5271,  0.2261,\n",
       "          0.0699,  0.5331,  0.9309,  0.9730,  0.1046,  0.6948,  0.3938,  0.5380]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.init_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(),'model1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x139d18580>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "imshow(data[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
